{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA CLEANING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# 1. Caricamento del file sorgente\n",
    "SOURCE_FILE = r\"EntryAccessoAmministrati_202501.csv\"\n",
    "\n",
    "df = pd.read_csv(SOURCE_FILE)\n",
    "\n",
    "# 2. Mappatura (IT ➜ EN) dei nomi colonna\n",
    "col_map = {\n",
    "    \"regione_residenza_domicilio\": \"region_of_residence\",\n",
    "    \"amministrazione_appartenenza\": \"administration\",\n",
    "    \"sesso\": \"gender\",\n",
    "    \"eta_max\": \"age_max\",\n",
    "    \"eta_min\": \"age_min\",\n",
    "    \"modalita_autenticazione\": \"Access_method\",\n",
    "    \"numero_occorrenze\": \"employee_count\",\n",
    "}\n",
    "\n",
    "df = df.rename(columns=col_map)\n",
    "\n",
    "# 3. Pulizia / conversione dei tipi\n",
    "#    ▸ eventuali stringhe vuote in 'age_max' diventano 0\n",
    "#    ▸ conversione a interi\n",
    "df[\"age_max\"] = (\n",
    "    pd.to_numeric(df[\"age_max\"], errors=\"coerce\")  # '34', ' ' … ➜ numerico / NaN\n",
    "    .fillna(0)                                     # NaN ➜ 0\n",
    "    .astype(int)\n",
    ")\n",
    "df[\"age_min\"] = df[\"age_min\"].astype(int)\n",
    "\n",
    "# 4. (Facoltativo) Ordine coerente delle colonne\n",
    "df = df[list(col_map.values())]\n",
    "\n",
    "output_file = r\"Accesso_English.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "\n",
    "SOURCE_FILE = r\"EntryAccreditoStipendi_202501.csv\"\n",
    "\n",
    "df = pd.read_csv(SOURCE_FILE)\n",
    "\n",
    "# 2. Mapping IT ➜ EN delle colonne\n",
    "col_map = {\n",
    "    \"comune_della_sede\":  \"office_municipality\",\n",
    "    \"amministrazione\":    \"administration\",\n",
    "    \"eta_min\":            \"age_min\",\n",
    "    \"eta_max\":            \"age_max\",\n",
    "    \"sesso\":              \"gender\",\n",
    "    \"modalita_pagamento\": \"payment_method\",\n",
    "    \"numero\":             \"salary_count\",\n",
    "}\n",
    "\n",
    "df = df.rename(columns=col_map)\n",
    "\n",
    "# 3. Pulizia / conversione dei tipi\n",
    "df[\"age_max\"] = (\n",
    "    pd.to_numeric(df[\"age_max\"], errors=\"coerce\")  # es. ' ' ➜ NaN\n",
    "      .fillna(0)                                   # NaN ➜ 0\n",
    "      .astype(int)\n",
    ")\n",
    "df[\"age_min\"]    = df[\"age_min\"].astype(int)\n",
    "df[\"salary_count\"] = df[\"salary_count\"].astype(int)\n",
    "\n",
    "# 4. (Facoltativo) Ordina le colonne come nel file di destinazione\n",
    "df = df[list(col_map.values())]\n",
    "\n",
    "output_file_stipendi = r\"Stipendi_English.csv\"\n",
    "df.to_csv(output_file_stipendi, index=False)\n",
    "\n",
    "import re\n",
    "\n",
    "SOURCE_FILE = r\"EntryAmministratiPerFasciaDiReddito_202501.csv\"\n",
    "\n",
    "df = pd.read_csv(SOURCE_FILE)\n",
    "\n",
    "# 2. Mappatura IT ➜ EN delle colonne\n",
    "col_map = {\n",
    "    \"comparto\":              \"sector\",\n",
    "    \"regione_residenza\":     \"region_of_residence\",\n",
    "    \"sesso\":                 \"gender\",\n",
    "    \"eta_min\":               \"age_min\",\n",
    "    \"eta_max\":               \"age_max\",\n",
    "    \"aliquota_max\":          \"max_tax_rate\",\n",
    "    \"fascia_reddito_min\":    \"income_bracket_min\",\n",
    "    \"fascia_reddito_max\":    \"income_bracket_max\",\n",
    "    \"numerosita\":            \"employee_count\",\n",
    "}\n",
    "df = df.rename(columns=col_map)\n",
    "\n",
    "# 3. Funzione di supporto: estrae la prima cifra da stringhe tipo\n",
    "#    \"Fino a 28.000\" o \"Oltre i 28000\"\n",
    "def extract_number(txt: str | float | int):\n",
    "    if pd.isna(txt):\n",
    "        return None\n",
    "    m = re.search(r\"(\\d[\\d\\.]*)\", str(txt))\n",
    "    if not m:\n",
    "        return None\n",
    "    return int(m.group(1).replace(\".\", \"\"))\n",
    "\n",
    "# 4. Parsing delle fasce di reddito\n",
    "df[\"income_bracket_min\"] = df[\"income_bracket_min\"].apply(extract_number)\n",
    "df[\"income_bracket_max\"] = df[\"income_bracket_max\"].apply(extract_number)\n",
    "\n",
    "#    ▸ se il minimo è mancante (riga con \"Fino a …\") lo impostiamo a 0\n",
    "df[\"income_bracket_min\"] = df[\"income_bracket_min\"].fillna(0).astype(int)\n",
    "\n",
    "# 5. Conversione e pulizia dei tipi numerici\n",
    "num_cols = [\"age_min\", \"age_max\", \"max_tax_rate\", \"income_bracket_max\", \"employee_count\"]\n",
    "for c in num_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "df[\"income_bracket_max\"] = df[\"income_bracket_max\"].astype(\"Int64\")   # può restare NaN\n",
    "df[\"employee_count\"] = df[\"employee_count\"].fillna(0).astype(int)\n",
    "\n",
    "# 6. (Facoltativo) Ordina le colonne come nel file di destinazione\n",
    "df = df[list(col_map.values())]\n",
    "\n",
    "output_file_reddito = r\"Reddito_English.csv\"\n",
    "df.to_csv(output_file_reddito, index=False)\n",
    "\n",
    "# 1. File di origine/destinazione\n",
    "SOURCE_FILE = r\"EntryPendolarismo_202501.csv\"\n",
    "\n",
    "df = pd.read_csv(SOURCE_FILE)\n",
    "\n",
    "# 2. Mapping IT ➜ EN dei nomi colonna\n",
    "col_map = {\n",
    "    \"provincia_della_sede\": \"province_of_office\",\n",
    "    \"comune_della_sede\":    \"office_municipality\",\n",
    "    \"stesso_comune\":        \"same_municipality\",\n",
    "    \"ente\":                 \"organization\",\n",
    "    \"numero_amministrati\":  \"employee_count\",\n",
    "    \"distance_min_KM\":      \"commuting_distance_min_km\",\n",
    "    \"distance_max_KM\":      \"commuting_distance_max_km\",\n",
    "}\n",
    "\n",
    "df = df.rename(columns=col_map)\n",
    "\n",
    "# 3. Conversioni e pulizia\n",
    "#    ▸ same_municipality: \"SI\"/\"NO\" ➜ boolean\n",
    "df[\"same_municipality\"] = df[\"same_municipality\"].str.upper().eq(\"SI\")\n",
    "\n",
    "#    ▸ distanza min/max ➜ interi\n",
    "for c in [\"commuting_distance_min_km\", \"commuting_distance_max_km\"]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "#    ▸ employee_count ➜ intero\n",
    "df[\"employee_count\"] = pd.to_numeric(df[\"employee_count\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# 4. nuova colonna: average_commuting_distance\n",
    "df[\"average_commuting_distance\"] = (\n",
    "    (df[\"commuting_distance_min_km\"] + df[\"commuting_distance_max_km\"]) / 2\n",
    ").round().astype(int)\n",
    "\n",
    "# 5. (Opzionale) Ordina le colonne come nel file di destinazione\n",
    "ordered_cols = list(col_map.values()) + [\"average_commuting_distance\"]\n",
    "df = df[ordered_cols]\n",
    "\n",
    "output_file_pendolarismo = r\"Pendolarismo_English.csv\"\n",
    "df.to_csv(output_file_pendolarismo, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accesso = pd.read_csv(r'Accesso_English.csv')\n",
    "\n",
    "# EDA sull'dataset accesso\n",
    "\n",
    "# Visualizziamo dimensioni, informazioni e valori mancanti\n",
    "print(\"Shape:\", accesso.shape)\n",
    "print(\"\\nInfo:\")\n",
    "accesso.info()\n",
    "print(\"\\nValori mancanti:\")\n",
    "print(accesso.isnull().sum())\n",
    "\n",
    "# Statistiche descrittive per le variabili numeriche e categoriche\n",
    "print(\"\\nStatistiche descrittive (numeriche):\")\n",
    "print(accesso.describe())\n",
    "print(\"\\nStatistiche descrittive (categoriche):\")\n",
    "print(accesso.describe(include=['object']))\n",
    "\n",
    "# Distribuzioni delle variabili categoriche: gender e region_of_residence\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(data=accesso, x='gender')\n",
    "plt.title(\"Distribuzione di gender\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(data=accesso, x='region_of_residence')\n",
    "plt.title(\"Distribuzione di region_of_residence\")\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Miglioramento dell'EDA sul dataset accesso\n",
    "\n",
    "# Distribuzione delle variabili numeriche: age_max, age_min, employee_count\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.histplot(accesso['age_max'], kde=True, bins=20, color='blue')\n",
    "plt.title(\"Distribuzione di age_max\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.histplot(accesso['age_min'], kde=True, bins=20, color='green')\n",
    "plt.title(\"Distribuzione di age_min\")\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(accesso['employee_count'], kde=False, bins=20, color='orange')\n",
    "plt.title(\"Distribuzione di employee_count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribuzione delle variabili categoriche: authentication_method e gender\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(data=accesso, x='authentication_method', palette='Set2')\n",
    "plt.title(\"Distribuzione di authentication_method\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(data=accesso, x='gender', palette='Set3')\n",
    "plt.title(\"Distribuzione di gender\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribuzione delle regioni con più occorrenze\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_regions = accesso['region_of_residence'].value_counts().head(10)\n",
    "sns.barplot(x=top_regions.index, y=top_regions.values, palette='viridis')\n",
    "plt.title(\"Top 10 regioni per occorrenze\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Occorrenze\")\n",
    "plt.xlabel(\"Regione\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "pendolarismo = pd.read_csv(r\"Pendolarismo_English.csv\")\n",
    "\n",
    "# EDA sul dataset \"pendolarismo\"\n",
    "\n",
    "# Informazioni di base e statistiche descrittive\n",
    "print(\"Shape:\", pendolarismo.shape)\n",
    "print(\"\\nInfo:\")\n",
    "pendolarismo.info()\n",
    "print(\"\\nValori mancanti:\")\n",
    "print(pendolarismo.isnull().sum())\n",
    "\n",
    "print(\"\\nStatistiche descrittive (numeriche):\")\n",
    "print(pendolarismo.describe())\n",
    "print(\"\\nStatistiche descrittive (categoriche):\")\n",
    "print(pendolarismo.describe(include=['object']))\n",
    "\n",
    "# Analisi grafica delle variabili numeriche\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.histplot(pendolarismo['commuting_distance_min_km'], kde=True, bins=20, color='blue')\n",
    "plt.title(\"Distribuzione di commuting_distance_min_km\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.histplot(pendolarismo['commuting_distance_max_km'], kde=True, bins=20, color='green')\n",
    "plt.title(\"Distribuzione di commuting_distance_max_km\")\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(pendolarismo['average_commuting_distance'], kde=True, bins=20, color='orange')\n",
    "plt.title(\"Distribuzione di average_commuting_distance\")\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.histplot(pendolarismo['employee_count'], kde=False, bins=20, color='red')\n",
    "plt.title(\"Distribuzione di employee_count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analisi delle variabili categoriche: conteggio per organization\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_org = pendolarismo['organization'].value_counts().head(10)\n",
    "sns.barplot(x=top_org.index, y=top_org.values, palette='viridis')\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Top 10 Organizations per numero di occorrenze\")\n",
    "plt.xlabel(\"Organization\")\n",
    "plt.ylabel(\"Occorrenze\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "reddito = pd.read_csv(r\"Reddito_English.csv\")\n",
    "\n",
    "# EDA sul dataset \"reddito\"\n",
    "\n",
    "# Informazioni di base e statistiche descrittive\n",
    "print(\"Shape:\", reddito.shape)\n",
    "print(\"\\nInfo:\")\n",
    "reddito.info()\n",
    "print(\"\\nValori mancanti:\")\n",
    "print(reddito.isnull().sum())\n",
    "\n",
    "print(\"\\nStatistiche descrittive (numeriche):\")\n",
    "print(reddito.describe())\n",
    "print(\"\\nStatistiche descrittive (categoriche):\")\n",
    "print(reddito.describe(include=['object']))\n",
    "\n",
    "# Analisi grafica delle variabili numeriche\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.histplot(reddito['age_min'], kde=True, bins=20, color='blue')\n",
    "plt.title(\"Distribuzione di age_min\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.histplot(reddito['age_max'].dropna(), kde=True, bins=20, color='green')\n",
    "plt.title(\"Distribuzione di age_max\")\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(reddito['income_bracket_min'], kde=True, bins=20, color='orange')\n",
    "plt.title(\"Distribuzione di income_bracket_min\")\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.histplot(reddito['income_bracket_max'].dropna(), kde=True, bins=20, color='red')\n",
    "plt.title(\"Distribuzione di income_bracket_max\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analisi delle variabili categoriche\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "sns.countplot(data=reddito, x='gender', palette='Set2')\n",
    "plt.title(\"Distribuzione di gender\")\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "top_sectors = reddito['sector'].value_counts().head(10)\n",
    "sns.barplot(x=top_sectors.index, y=top_sectors.values, palette='viridis')\n",
    "plt.title(\"Top 10 settori per occorrenze\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Occorrenze\")\n",
    "plt.xlabel(\"Settore\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analisi delle regioni con più occorrenze\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_regions_reddito = reddito['region_of_residence'].value_counts().head(10)\n",
    "sns.barplot(x=top_regions_reddito.index, y=top_regions_reddito.values, palette='viridis')\n",
    "plt.title(\"Top 10 regioni per occorrenze\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Occorrenze\")\n",
    "plt.xlabel(\"Regione\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "stipendi = pd.read_csv(r\"Stipendi_English.csv\")\n",
    "\n",
    "# EDA sul dataset \"stipendi\"\n",
    "\n",
    "# Informazioni di base\n",
    "print(\"Shape:\", stipendi.shape)\n",
    "print(\"\\nInfo:\")\n",
    "stipendi.info()\n",
    "print(\"\\nValori mancanti:\")\n",
    "print(stipendi.isnull().sum())\n",
    "\n",
    "print(\"\\nStatistiche descrittive (numeriche):\")\n",
    "print(stipendi.describe())\n",
    "\n",
    "print(\"\\nStatistiche descrittive (categoriche):\")\n",
    "print(stipendi.describe(include=['object']))\n",
    "\n",
    "# Visualizzazione grafica\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Distribuzione di age_min\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.histplot(stipendi['age_min'], kde=True, bins=20, color='blue')\n",
    "plt.title(\"Distribuzione di age_min\")\n",
    "\n",
    "# Distribuzione di age_max\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.histplot(stipendi['age_max'], kde=True, bins=20, color='green')\n",
    "plt.title(\"Distribuzione di age_max\")\n",
    "\n",
    "# Distribuzione di gender\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.countplot(data=stipendi, x='gender', palette='Set2')\n",
    "plt.title(\"Distribuzione di gender\")\n",
    "\n",
    "# Distribuzione di payment_method\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.countplot(data=stipendi, x='payment_method', palette='Set3')\n",
    "plt.title(\"Distribuzione di payment_method\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analisi della distribuzione di salary_count\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(stipendi['salary_count'], kde=False, bins=20, color='purple')\n",
    "plt.title(\"Distribuzione di salary_count\")\n",
    "plt.xlabel(\"Salary Count\")\n",
    "plt.ylabel(\"Frequenza\")\n",
    "plt.show()\n",
    "\n",
    "# access:\n",
    "# • The distribution of ages (age_min and age_max) appears relatively symmetric,\n",
    "#   suggesting a homogeneous coverage of age groups. However, it is necessary to further check\n",
    "#   if there are outliers at the extremes that could influence subsequent analytical models.\n",
    "# • The distribution of 'employee_count' varies significantly, indicating that some administrations\n",
    "#   manage much larger structures compared to others.\n",
    "# • The authentication methods (SPID vs. CIE) highlight a predominance of SPID, although\n",
    "#   isolated cases of other methodologies could indicate particular authentication processes.\n",
    "\n",
    "# commuting:\n",
    "# • The distributions of variables related to distances (commuting_distance_min_km, commuting_distance_max_km, and average_commuting_distance)\n",
    "#   show high dispersion. The presence of very high values, for example in commuting_distance_max_km,\n",
    "#   could indicate outliers or exceptionally long routes.\n",
    "# • The strong peak in 'employee_count' for some organizations (such as the MINISTRY OF EDUCATION AND MERIT)\n",
    "#   suggests that, for these institutions, the commuting effect is particularly significant.\n",
    "\n",
    "# income:\n",
    "# • The age distribution shows clustering in the central groups while, in some cases,\n",
    "#   the presence of missing values in age_max and income_bracket_max requires careful handling of these variables.\n",
    "# • The distributions of income_bracket indicate that most incomes fall into lower brackets,\n",
    "#   supporting the idea of a concentration around fixed thresholds (e.g., income up to €28,000).\n",
    "# • The analysis by 'sector' and 'gender' reveals potential disparities in sectors and gender distributions,\n",
    "#   highlighting possible patterns related to specific roles in certain work areas.\n",
    "# • Regions such as LAZIO and CAMPANIA appear predominant, which may result from greater centralization of services\n",
    "#   or a more significant demographic presence in these areas.\n",
    "\n",
    "# salaries:\n",
    "# • The distributions for age_min and age_max highlight consistent groups in the age ranges, indicating a certain standardization\n",
    "#   in age-based pay policies.\n",
    "# • The predominance of the payment_method \"Bank account/Account card\" suggests strong standardization in payment systems,\n",
    "#   while any anomalies in other methods may warrant further investigation.\n",
    "# • The variability in the distribution of salary_count suggests that the number of payments (or their frequency)\n",
    "#   varies significantly between administrations, potentially influenced by organizational differences and employment contracts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AGENTS CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner, ModelSettings, function_tool, InputGuardrail, GuardrailFunctionOutput\n",
    "#from agents.extensions.visualization import draw_graph\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "import asyncio\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import os\n",
    "import io\n",
    "import contextlib\n",
    "\n",
    "# Paste your real API key here (you can also use getpass for extra security)\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "import openai\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Load datasets from uploaded files\n",
    "df_accesso = pd.read_csv(\"Accesso_English.csv\")\n",
    "df_stipendi = pd.read_csv(\"Stipendi_English.csv\")\n",
    "df_reddito = pd.read_csv(\"Reddito_English.csv\")\n",
    "df_pendolarismo = pd.read_csv(\"Pendolarismo_English.csv\")\n",
    "import os\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, load_index_from_storage\n",
    "from llama_index.core.storage.storage_context import StorageContext\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from dotenv import load_dotenv\n",
    "from agents import function_tool\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Load embedding model\n",
    "embedding_model = OpenAIEmbedding()\n",
    "\n",
    "# Assume df_accesso, df_pendolarismo, df_reddito, df_stipendi are already loaded globally\n",
    "dataset_info = {\n",
    "    \"accesso\": {\n",
    "        \"description\": \"Region of residence, type of administration, gender, age groups, authentication methods (e.g., SPID), and total number of employees per group\",\n",
    "        \"dataframe\": df_accesso\n",
    "    },\n",
    "    \"pendolarismo\": {\n",
    "        \"description\": \"Commuting distances (min, max, average), province and municipality of the office, organization name, whether residence matches office location, and number of commuting employees\",\n",
    "        \"dataframe\": df_pendolarismo\n",
    "    },\n",
    "    \"reddito\": {\n",
    "        \"description\": \"Income bracket ranges, maximum tax rates, employee age and gender distribution, economic sector, region of residence, and employee count per bracket\",\n",
    "        \"dataframe\": df_reddito\n",
    "    },\n",
    "    \"stipendi\": {\n",
    "        \"description\": \"Payment methods, salary counts, gender and age ranges, type of administration, and office municipality\",\n",
    "        \"dataframe\": df_stipendi\n",
    "    }\n",
    "}\n",
    "\n",
    "@function_tool\n",
    "def processo_data(query: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Select the most relevant dataset and columns based on a natural-language query, extract and compute all necessary data points or aggregates, and provide a clear, data-driven answer by following this updated developer workflow using LlamaIndex for document-level understanding.\n",
    "\n",
    "    1. Explicitly State the Goal\n",
    "       - Retrieve and interpret relevant information from the four datasets to answer the user’s query with clarity and correctness.\n",
    "\n",
    "    2. Deeply Understand the Query\n",
    "       - Analyze the user’s intent, identifying: metrics, comparisons, dimensions, filters, and any implicit requests (e.g. grouping, distributions, summaries).\n",
    "\n",
    "    3. Investigate Available Data\n",
    "       - Work only with these datasets:\n",
    "        -\"Accesso_English.csv\"\n",
    "        -\"Stipendi_English.csv\"\n",
    "        -\"Reddito_English.csv\"\n",
    "        -\"Pendolarismo_English.csv\"\n",
    "       - Reference the ⁠ dataset_info ⁠ dictionary for schema descriptions.\n",
    "       - Always validate that referenced columns exist and data types match expectations.\n",
    "\n",
    "    4. Plan Data Extraction and Computation\n",
    "       - Based on user intent, determine whether the output requires raw data, aggregated values (sum, mean, count), distributions, or comparisons.\n",
    "       - Identify possible grouping variables (e.g., region, gender, office location) and appropriate filters (e.g., high income, long commute).\n",
    "\n",
    "    5. Implement Matching Logic\n",
    "       - Use LlamaIndex to embed and semantically compare the *full dataset descriptions as documents* with the query.\n",
    "       - Avoid sentence-level transformers; leverage the semantic document matching capabilities of LlamaIndex.\n",
    "       - Select the most relevant dataset based on document-level similarity scoring.\n",
    "       - Within the selected dataset, semantically match the best numeric column (and grouping column, if needed).\n",
    "\n",
    "    6. Handle Non-Specific or Ambiguous Queries\n",
    "       - If key query elements are vague, missing, or unresolvable (e.g., column not found, unclear metric), return an error message.\n",
    "       - Always explain why the query is insufficient and offer the user the chance to reformulate their question with clearer details.\n",
    "\n",
    "    7. Generate the Data-Driven Prompt\n",
    "       - Formulate a concise, well-structured prompt that will enable the \"generate_python_code\" tool to correctly compute the result or generate the appropriate plot.\n",
    "       - Include selected dataset, relevant columns, type of metric, and any grouping logic if applicable.\n",
    "       - Make sure that when you call dataset and columns they are in quotes, so the \"generate_python_code\" tool will correctly understand.\n",
    "\n",
    "    8. Validate and Test\n",
    "       - Ensure that the chosen \"dataset_name\" is valid.\n",
    "       - Ensure that the chosen columns exist in the dataset, if not reiterate the process.\n",
    "       - Verify referenced column names against actual dataframe columns.\n",
    "       - Make sure that when you call dataset and columns they are in quotes, so the \"generate_python_code\" tool will correctly understand.\n",
    "       - Confirm all requested operations are feasible with the selected data.\n",
    "       - Handle edge cases (e.g., empty results, lack of numeric fields) with graceful error messages.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1 – Embed the query\n",
    "    query_embedding = embedding_model.get_query_embedding(query)\n",
    "\n",
    "    # Step 2 – Select dataset by semantic similarity\n",
    "    dataset_scores = {\n",
    "        name: float(embedding_model.similarity(query_embedding, embedding_model.get_text_embedding(info[\"description\"])))\n",
    "        for name, info in dataset_info.items()\n",
    "    }\n",
    "    best_dataset = max(dataset_scores, key=dataset_scores.get)\n",
    "    df = dataset_info[best_dataset][\"dataframe\"]\n",
    "\n",
    "    # Step 3 – Select best numeric column (fallback if none)\n",
    "    numeric_cols = df.select_dtypes(include=\"number\").columns.tolist()\n",
    "    if not numeric_cols:\n",
    "        return {\n",
    "            \"error\": f\"No numeric columns found in the selected dataset '{best_dataset}'. Please refine your query.\"\n",
    "        }\n",
    "\n",
    "    similarities = {\n",
    "        col: float(embedding_model.similarity(query_embedding, embedding_model.get_text_embedding(col)))\n",
    "        for col in numeric_cols\n",
    "    }\n",
    "    best_numeric = max(similarities, key=similarities.get)\n",
    "\n",
    "    # Step 4 – Optionally select group-by column\n",
    "    best_groupby = None\n",
    "    if \" by \" in query.lower() or \"group\" in query.lower():\n",
    "        categorical_cols = df.select_dtypes(include=\"object\").columns.tolist()\n",
    "        if categorical_cols:\n",
    "            cat_similarities = {\n",
    "                col: float(embedding_model.similarity(query_embedding, embedding_model.get_text_embedding(col)))\n",
    "                for col in categorical_cols\n",
    "            }\n",
    "            best_groupby = max(cat_similarities, key=cat_similarities.get)\n",
    "\n",
    "    # Step 5 – Return structured info\n",
    "    return {\n",
    "        \"dataset_name\": best_dataset,\n",
    "        \"column\": best_numeric,\n",
    "        \"group_by\": best_groupby,\n",
    "        \"columns\": list(df.columns),\n",
    "        \"description\": (\n",
    "            f\"Matched to '{best_dataset}' with similarity score {round(dataset_scores[best_dataset], 4)}.\\n\"\n",
    "            f\"Selected numeric column: '{best_numeric}'.\\n\"\n",
    "            f\"{'Group by: ' + best_groupby if best_groupby else 'No group-by column identified.'}\"\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "@function_tool\n",
    "def generate_python_code(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate Python code from a natural-language prompt, following a clear developer workflow:\n",
    "\n",
    "    1. Explicitly State the Goal\n",
    "       - Understand whether the user wants data loading, transformation, analysis, modeling, or other tasks.\n",
    "\n",
    "    2. Deeply Understand the Prompt\n",
    "       - Parse the user’s intent: key operations, inputs, outputs, libraries required.\n",
    "       - If any requirement is unclear or under‑specified, ask the user for clarification.\n",
    "\n",
    "    3. Handle Ambiguity\n",
    "\n",
    "       - For vague requests, request details (e.g. variable names, file paths, function signatures).\n",
    "       - If the column called by the processo_data tool doesn't exist replace it with the most similar column\n",
    "\n",
    "    4. Context: Available Datasets\n",
    "       - Four CSVs are available under these paths:\n",
    "        • \"Accesso_English.csv\"\n",
    "        • \"Pendolarismo_English.csv\"\n",
    "        • \"Reddito_English.csv\"\n",
    "        • \"Stipendi_English.csv\"\n",
    "       - If the prompt refers to data from any of these files, verify in code that the referenced columns exist in the chosen dataset before using them.\n",
    "\n",
    "    5. Plan Code Structure\n",
    "       - Identify necessary imports (e.g. pandas, numpy, matplotlib, scikit-learn).\n",
    "       - Outline function definitions or script sections: data loading, preprocessing, computation, output.\n",
    "\n",
    "    6. Handle the marge:\n",
    "        Given two CSV files and a natural‑language question about the relationship between a “feature” in the first file and a “target” in the second, automatically:\n",
    "\n",
    "        1. *Load* both files into pandas DataFrames, e.g. ⁠ df1 ⁠, ⁠ df2 ⁠.\n",
    "        2. *Inspect* their schemas:\n",
    "          a. Print ⁠ df1.columns ⁠ and ⁠ df2.columns ⁠.\n",
    "          b. Identify possible join keys by looking for identical or semantically matching column names (e.g. “administration” vs. “organization”).\n",
    "          c. If multiple candidates exist, choose the one with the greatest overlap of non‑null values. Rename both sides to a common key name ⁠ KEY ⁠.\n",
    "        3. *Determine*:\n",
    "          - In ⁠ df1 ⁠, which column(s) correspond to the “feature” (categorical or numeric) mentioned in the question.\n",
    "          - In ⁠ df2 ⁠, which column is the “target” numeric variable (e.g. ⁠ average_commuting_distance ⁠).\n",
    "        4. *Aggregate*:\n",
    "          - If the feature is categorical, group ⁠ df1 ⁠ by ⁠ KEY ⁠ and that feature, sum or count as appropriate, pivot to one‑row‑per‑⁠ KEY ⁠ with proportions for each category.\n",
    "          - If it’s numeric, compute the mean (or other summary) per ⁠ KEY ⁠.\n",
    "        5. *Aggregate* the target in ⁠ df2 ⁠ to one row per ⁠ KEY ⁠ (e.g. weighted average if there’s a count column).\n",
    "        6. If necessary given the request, *Merge* the two aggregated tables on ⁠ KEY ⁠, dropping rows where either side is missing.\n",
    "            - for example, if the merge is between df_pendolarismo and df_accesso, merge on administration.\n",
    "        7. *Compute* the statistic your question asks for (e.g. Pearson correlation between each feature‑column and the target; or a difference‑of‑means for two groups).\n",
    "        8. *Plot* scatterplots or bar charts to illustrate each relationship.\n",
    "        9. *Return*:\n",
    "          - The Python code implementing steps 1–8.\n",
    "          - A brief text summary stating the main numeric results (e.g. “Correlation between X and Y is 0.12, indicating no strong linear relationship.”).\n",
    "\n",
    "        Use only the two provided CSV filepaths; do not load any other data.\n",
    "\n",
    "    7. Implement Code\n",
    "       - Write idiomatic, well-commented Python.\n",
    "       - Use pandas for data handling; validate column existence via ⁠ if col in df.columns: ⁠ checks when reading these datasets.\n",
    "       - Organize into functions or a runnable script as appropriate.\n",
    "\n",
    "    8. Validate and Test\n",
    "       - Ensure the generated code runs without errors.\n",
    "       - Confirm that any dataset and column references are guarded by existence checks.\n",
    "       - Include brief test snippets or assertions to verify correctness where applicable.\n",
    "\n",
    "    9. Deliver the Code\n",
    "       - Return the full Python source as a single string, ready to execute.\n",
    "    \"\"\"\n",
    "\n",
    "import io\n",
    "import contextlib\n",
    "\n",
    "@function_tool\n",
    "def execute_code_text(code: str) -> dict:\n",
    "    \"\"\"\n",
    "    Executes Python code and returns:\n",
    "      - ⁠ output ⁠: everything printed during execution\n",
    "    \"\"\"\n",
    "    buf_out = io.StringIO()\n",
    "    try:\n",
    "        ns = {\n",
    "            \"__builtins__\": __builtins__,\n",
    "            'pd': pd,\n",
    "            'plt': plt,\n",
    "            'os': os,\n",
    "            'sns': sns,\n",
    "            'display': lambda obj: print(obj),\n",
    "            'df_accesso': df_accesso,\n",
    "            'df_pendolarismo': df_pendolarismo,\n",
    "            'df_reddito': df_reddito,\n",
    "            'df_stipendi': df_stipendi,\n",
    "        }\n",
    "\n",
    "        with contextlib.redirect_stdout(buf_out):\n",
    "            exec(code, ns)\n",
    "\n",
    "        return {\n",
    "            'output': buf_out.getvalue().strip(),\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        return {\n",
    "            'output': f\"❌ Error: {e}\\n{traceback.format_exc()}\",\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "# Memory buffer setup\n",
    "memory_buffer = []\n",
    "max_memory_length = 6\n",
    "\n",
    "def update_memory_buffer(memory_buffer, user_prompt: str, agent_response: str):\n",
    "    memory_buffer.append({\n",
    "        \"user\": user_prompt,\n",
    "        \"agent\": agent_response\n",
    "    })\n",
    "    if len(memory_buffer) > max_memory_length:\n",
    "        memory_buffer.pop(0)\n",
    "\n",
    "async def triage_with_memory(user_query: str, agent, memory_buffer: list, max_memory_length=6):\n",
    "    \"\"\"\n",
    "    Enhances the prompt with summarized memory,\n",
    "    detects duplicate user questions, runs the agent,\n",
    "    and logs both the question and the answer as structured memory.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Normalize current query\n",
    "    normalized_query = user_query.strip().lower()\n",
    "\n",
    "    # Step 2: Detect repeat (if memory has structured entries)\n",
    "    if all(isinstance(entry, dict) for entry in memory_buffer):\n",
    "        user_prompts = [entry[\"user\"].strip().lower() for entry in memory_buffer]\n",
    "    else:\n",
    "        user_prompts = [\n",
    "            entry.split(\"\\n\")[0].replace(\"[User] \", \"\").strip().lower()\n",
    "            for entry in memory_buffer\n",
    "            if isinstance(entry, str)\n",
    "        ]\n",
    "\n",
    "    if normalized_query in user_prompts:\n",
    "        print(\"⚠️ You've already asked this! The agent will still respond, but consider rephrasing.\")\n",
    "\n",
    "    # Step 3: Build summarized context\n",
    "    summarized_context = \"Conversation so far:\\n\"\n",
    "    for entry in memory_buffer[-3:]:\n",
    "        if isinstance(entry, dict):\n",
    "            summarized_context += f\"- User asked: {entry['user']} → Agent answered: {entry['agent']}\\n\"\n",
    "        elif isinstance(entry, str):\n",
    "            summarized_context += f\"- {entry}\\n\"\n",
    "\n",
    "    summarized_context += f\"\\nNow the user asks:\\n\\\"{user_query}\\\"\"\n",
    "\n",
    "    # Step 4: Run agent\n",
    "    result = await Runner.run(agent, summarized_context)\n",
    "    response = result.final_output\n",
    "\n",
    "    # Step 5: Log interaction as structured memory\n",
    "    memory_buffer.append({\n",
    "        \"user\": user_query,\n",
    "        \"agent\": response\n",
    "    })\n",
    "\n",
    "    # Step 6: Return answer\n",
    "    return response\n",
    "\n",
    "\n",
    "\n",
    "Reporter_agent = Agent(\n",
    "    name=\"Reporter\",\n",
    "    instructions=(\n",
    "        \"You are the Reporter (visualization/code-gen agent). You will receive a prompt *only* from Data Processor. \"\n",
    "        \"Treat that prompt as authoritative. \"\n",
    "        \"1. First, use the ⁠ generate_python_code ⁠ tool exactly once to produce the final Python code or report.  \\n\"\n",
    "        \"2. Then, immediately pass that generated code to the ⁠ execute_code_text ⁠ tool to run it and capture its output.  \\n\"\n",
    "        \"3. Return a single response containing both:  \\n\"\n",
    "        \"   • The execution results (stdout or error) from ⁠ execute_code_text ⁠.  \\n\"\n",
    "        \"   • The key Insight of the obtained result.  \\n\"\n",
    "        \"Do *not* re‑interpret the user’s original query or call any other tools.\"\n",
    "    ),\n",
    "    model=\"gpt-4.1\",\n",
    "    tools=[generate_python_code, execute_code_text]\n",
    ")\n",
    "data_processor = Agent(\n",
    "    name=\"Data Processor\",\n",
    "    instructions=(\n",
    "        \"You are the Data Processor. Always start by calling the ⁠ processo_data ⁠ tool with the user’s query. \"\n",
    "        \"Wait for its structured JSON response. \"\n",
    "        \"Then, transform that JSON into a clear, concise natural-language prompt for the Reporter agent \"\n",
    "        \"(mention dataset name, chosen column, group-by if any, and what analysis to perform). \"\n",
    "        \"Hand off that new prompt to Reporter. Do *not* answer the user directly.\"\n",
    "    ),\n",
    "    model=\"gpt-4.1\",\n",
    "    handoffs=[Reporter_agent],\n",
    "    tools=[processo_data]\n",
    ")\n",
    "\n",
    "\n",
    "triage_agent = Agent(\n",
    "    name=\"ConversationAgent\",\n",
    "    instructions=(\n",
    "        \"You are the orchestrator. For every incoming user query, do *not* answer directly. \"\n",
    "        \"Instead, immediately call the ⁠ processo_data ⁠ tool (via the Data Processor agent) with the raw user query. \"\n",
    "        \"Do *not* attempt any processing yourself, and do *not* call any other tool or return any text. \"\n",
    "        \"Simply package the user’s exact query as input to Data Processor.\"\n",
    "    ),\n",
    "    model=\"gpt-4.1\",\n",
    "    handoffs=[data_processor],\n",
    "    tools=[]\n",
    ")\n",
    "\n",
    "#draw_graph(triage_agent)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
